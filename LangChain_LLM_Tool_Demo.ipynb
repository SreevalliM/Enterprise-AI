{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoeWzTx4DHUm"
   },
   "source": [
    "# LLMs and their tools\n",
    "\n",
    "## Objective:\n",
    "LLMs don't execute tools directly - they only decide which tools to use and parse arguments. The actual tool execution happens in separate code.\n",
    "\n",
    "This notebook compares two approaches for integrating tools with LLMs:\n",
    "\n",
    "1. Manual Tool Chaining (explicit parsing/execution)\n",
    "\n",
    "2. llm.bind_tools() (automatic tool-call binding).\n",
    "\n",
    "With bind_tools(), this process is automated through structured outputs, while manual chaining gives you control by parsing text responses like 'TOOL: calculator|2+2' and routing to functions yourself.\n",
    "\n",
    "Weâ€™ll explore the trade-offs between these methods, including how each interacts with RunnableWithMessageHistory for stateful conversations. Below is a high-level comparison:\n",
    "\n",
    "| Aspect               | Manual Tool Chaining                        | `bind_tools()`                                   |\n",
    "|----------------------|---------------------------------------------|--------------------------------------------------|\n",
    "| **Code Clarity**     | More complex, error-prone parsing           | Cleaner, automatic JSON-based tool calls         |\n",
    "| **Tool Execution**   | Manual (logic in prompts/code)              | Decoupled: LLM suggests calls, code executes     |\n",
    "| **Memory Support**   | Native via message history wrapper          | Requires manual tool-result logging              |\n",
    "| **Flexibility**      | Full control over workflow                  | Structured (limited by API design)               |\n",
    "| **Best For**         | Custom workflows, complex logic             | Rapid development, standardized tooling          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0o6Dq5bvDHUp",
    "outputId": "d16e0603-8e76-439d-dd0c-54ada08c03c6"
   },
   "outputs": [],
   "source": [
    "! pip install langchain==0.3.25 langgraph==0.4.5 langchain-openai==0.3.18 python-dotenv==1.1.0 gradio==5.17.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egHVRj_BDHUr"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jsvz8rjpDHUr"
   },
   "outputs": [],
   "source": [
    "## Common Setup\n",
    "import os\n",
    "from datetime import datetime\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "import logging\n",
    "\n",
    "load_dotenv()\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# Shared tool definitions\n",
    "@tool\n",
    "def calculator_tool(expression: str) -> float:\n",
    "    \"\"\"Calculate mathematical expressions safely.\"\"\"\n",
    "    try:\n",
    "        # Simple safe evaluation (in production, use a proper math parser)\n",
    "        result = eval(expression.replace(\"^\", \"**\"))\n",
    "        return float(result)\n",
    "    except:\n",
    "        return \"Error: Invalid expression\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_current_time(format_str: str = \"%Y-%m-%d %H:%M:%S\") -> str:\n",
    "    \"\"\"Get current date and time in specified format.\"\"\"\n",
    "    return datetime.now().strftime(format_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ibd1Fj-DHUs"
   },
   "source": [
    "## Lets compare Manual Chaining vs Bind tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9vjVFN-DHUs"
   },
   "source": [
    "## Approach 1 : Manual Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXtsrwK9DHUs"
   },
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "llm_for_manual_chain = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Step 1: Define prompt template for tool selection\n",
    "prompt_for_llm_for_manual_chain = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Analyze the user's question and decide if tools are needed.\n",
    "    Available tools:\n",
    "    - calculator_tool: For math operations (input: math expression)\n",
    "    - get_current_time: Returns current time (input: time format string)\n",
    "\n",
    "    Respond EXACTLY in one of these formats:\n",
    "\n",
    "    TOOL: calculator_tool|2+2\n",
    "    TOOL: get_current_time|%Y-%m-%d\n",
    "    ANSWER: direct_answer (e.g., \"ANSWER: The capital of France is Paris\")\n",
    "\n",
    "    User question: {question}\"\"\"\n",
    ")\n",
    "\n",
    "# Tool mapping dictionary\n",
    "tool_map = {\n",
    "    'calculator_tool': calculator_tool,\n",
    "    'get_current_time': get_current_time\n",
    "}\n",
    "\n",
    "def execute_tools_for_manual_chain(response: AIMessage) -> str:\n",
    "    \"\"\"Execute tools based on LLM response or return direct answer.\"\"\"\n",
    "    content = response.content\n",
    "    print(content)\n",
    "\n",
    "    if content.startswith(\"TOOL:\"):\n",
    "        try:\n",
    "            # Parse tool call\n",
    "            _, tool_call = content.split(\":\", 1)\n",
    "            tool_name, args = tool_call.strip().split(\"|\")\n",
    "\n",
    "            # Execute tool\n",
    "            tool = tool_map[tool_name]\n",
    "            result = tool.invoke(args.strip())\n",
    "            return f\"{result}\"\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Error executing tool: {str(e)}\"\n",
    "\n",
    "    elif content.startswith(\"ANSWER:\"):\n",
    "        # Return direct answer\n",
    "        return content.split(\":\", 1)[1].strip()\n",
    "\n",
    "    else:\n",
    "        return \"I couldn't process your request properly.\"\n",
    "\n",
    "# Step 2: Create the chain\n",
    "manual_chain = (\n",
    "    prompt_for_llm_for_manual_chain\n",
    "    | llm_for_manual_chain\n",
    "    | RunnableLambda(execute_tools_for_manual_chain)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fvudQiSgDHUt",
    "outputId": "d161ff75-1abc-4862-96b1-e3005b90239b"
   },
   "outputs": [],
   "source": [
    "# Test 1: Date query\n",
    "print(\"Query: What's today's date?\")\n",
    "response = manual_chain.invoke({\"question\": \"What's today's date?\"})\n",
    "print(f\"Response: {response}\\n\")\n",
    "\n",
    "# Test 2: Math query\n",
    "print(\"Query: What's 4+4?\")\n",
    "response = manual_chain.invoke({\"question\": \"What's 4+4?\"})\n",
    "print(f\"Response: {response}\\n\")\n",
    "\n",
    "# Non tool call\n",
    "print(\"Query: What's captial of UK\")\n",
    "response3 = manual_chain.invoke({\"question\":\"What's captial of UK\"})\n",
    "print(response3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPUhk6I9DHUu"
   },
   "source": [
    "## Approach 2 : LLM for bind tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RXDqfpyADHUu"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "llm_with_bind_tools = ChatOpenAI(temperature=0)\n",
    "tools = [calculator_tool, get_current_time]\n",
    "prompt_for_llm_with_bind_tools = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Analyze the user's question and use if tools are needed. Otherwise just answer the question\n",
    "    User question: {question}\"\"\"\n",
    ")\n",
    "\n",
    "# Tool mapping dictionary\n",
    "tool_map = {\n",
    "    'calculator_tool': calculator_tool,\n",
    "    'get_current_time': get_current_time\n",
    "}\n",
    "\n",
    "def execute_tools_for_bind_tools(response: AIMessage) -> str:\n",
    "    \"\"\"Execute tools based on LLM response or return direct answer.\"\"\"\n",
    "    tool_calls = response.additional_kwargs.get(\"tool_calls\", [])\n",
    "\n",
    "    for call in tool_calls:\n",
    "        function_call = call['function']\n",
    "        # Correct way to get tool name and arguments\n",
    "        tool_name = function_call[\"name\"]\n",
    "        tool_args = json.loads(function_call[\"arguments\"])\n",
    "\n",
    "        # Run the tool manually with keyword args unpacked\n",
    "        if tool_name == \"calculator_tool\":\n",
    "            output = calculator_tool.invoke(input=tool_args)\n",
    "        elif tool_name == \"get_current_time\":\n",
    "            output = get_current_time.invoke(input=tool_args)\n",
    "        else:\n",
    "            output = \"Tool not found.\"\n",
    "        return output\n",
    "    return response.content\n",
    "\n",
    "\n",
    "# Step 2: Create the chain\n",
    "bind_tools_chain = (\n",
    "    prompt_for_llm_with_bind_tools\n",
    "    | llm_with_bind_tools.bind_tools(tools) | RunnableLambda(execute_tools_for_bind_tools)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4fi0eUIXDHUu",
    "outputId": "2c156b75-0d61-49d8-bc78-5d18ae1564d3"
   },
   "outputs": [],
   "source": [
    "# Test 1: Date query\n",
    "print(\"Query: What's today's date?\")\n",
    "response = bind_tools_chain.invoke({\"question\": \"What's today's date?\"})\n",
    "print(f\"Response: {response}\\n\")\n",
    "\n",
    "# Test 2: Math query\n",
    "print(\"Query: What's 4+4?\")\n",
    "response = bind_tools_chain.invoke({\"question\": \"What's 4+4?\"})\n",
    "print(f\"Response: {response}\\n\")\n",
    "\n",
    "# Non tool call\n",
    "print(\"Query: What's captial of UK\")\n",
    "response3 = bind_tools_chain.invoke({\"question\":\"What's captial of UK\"})\n",
    "print(response3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5K8ET8KDHUv"
   },
   "source": [
    "## 3. Adding Memory to Manual Chain and Bind Tools Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79zc5yVhDHUv"
   },
   "source": [
    "### Adding Memory to Manual Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIptOnoUDHUv"
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "class ManualToolChaining:\n",
    "    \"\"\"Explicit tool handling with manual chaining and session memory.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize with the pre-built manual chain.\"\"\"\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are a helpful assistant that MUST follow the exact response format.\n",
    "\n",
    "        Available tools:\n",
    "        - calculator_tool: For mathematical calculations (any math expression)\n",
    "        - get_current_time: For current date/time (use format like %Y-%m-%d %H:%M:%S)\n",
    "\n",
    "        CRITICAL: You MUST respond in EXACTLY one of these formats:\n",
    "\n",
    "        For tool usage:\n",
    "        Always use all caps TOOL\n",
    "        TOOL: calculator_tool|2+2\n",
    "        TOOL: get_current_time|%Y-%m-%d %H:%M:%S\n",
    "\n",
    "        For direct answers:\n",
    "        ANSWER: Your direct response here\n",
    "\n",
    "        EXAMPLES:\n",
    "        - User asks \"What's the date?\" â†’ Respond: \"TOOL: get_current_time|%Y-%m-%d\"\n",
    "        - User asks \"What's 5+3?\" â†’ Respond: \"TOOL: calculator_tool|5+3\"\n",
    "        - User asks \"What's the capital of France?\" â†’ Respond: \"ANSWER: The capital of France is Paris\"\n",
    "        - User asks about previous conversation â†’ Look at the conversation history and respond: \"ANSWER: [summary of what they asked]\"\n",
    "\n",
    "        DO NOT give conversational responses like \"I can help you with that\" - use the exact format above.\"\"\"),\n",
    "                    MessagesPlaceholder(variable_name=\"history\"),\n",
    "                    (\"human\", \"{question}\")\n",
    "                ])\n",
    "\n",
    "        self.manual_chain = self.prompt | llm_for_manual_chain | RunnableLambda(execute_tools_for_manual_chain)\n",
    "        self.store = {}\n",
    "\n",
    "    def get_session_history(self, session_id: str) -> BaseChatMessageHistory:\n",
    "        \"\"\"Get or create session history for the given session ID.\"\"\"\n",
    "        if session_id not in self.store:\n",
    "            self.store[session_id] = ChatMessageHistory()\n",
    "        return self.store[session_id]\n",
    "\n",
    "    def process_message(self, message: str, session_id: str = \"default\") -> str:\n",
    "        \"\"\"Process message with manual tool chaining and memory.\"\"\"\n",
    "        try:\n",
    "            chain_with_memory = RunnableWithMessageHistory(\n",
    "                self.manual_chain,\n",
    "                self.get_session_history,\n",
    "                input_messages_key=\"question\",\n",
    "                history_messages_key=\"history\"\n",
    "            )\n",
    "\n",
    "            # Invoke the chain\n",
    "            response = chain_with_memory.invoke(\n",
    "                {\"question\": message},\n",
    "                config={\"configurable\": {\"session_id\": session_id}}\n",
    "            )\n",
    "\n",
    "            return response\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Error processing message: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2c7eTikDHUv",
    "outputId": "cd4e0f2b-b7d3-4de1-9263-437d307f70f6"
   },
   "outputs": [],
   "source": [
    "manual_chain_with_memory = ManualToolChaining()\n",
    "# Test 1: Date query\n",
    "print(\"Query: What's today's date?\")\n",
    "response = manual_chain_with_memory.process_message(message =\"What's date\", session_id =\"manual_session\")\n",
    "print(f\"Response: {response}\\n\")\n",
    "\n",
    "# Test 2: Math query\n",
    "print(\"Query: What's 4+4?\")\n",
    "response = manual_chain_with_memory.process_message(message =\"What's 4+4?\", session_id =\"manual_session\")\n",
    "print(f\"Response: {response}\\n\")\n",
    "\n",
    "# Non tool call\n",
    "print(\"Query: Add 4 to previous answer\")\n",
    "response = manual_chain_with_memory.process_message(message =\"Add 4 to previous answer\", session_id =\"manual_session\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvJ0ZI0vDHUw"
   },
   "source": [
    "### Adding Memory to Bind Tools Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "884Bu1WfDHUw"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.messages import AIMessage, ToolMessage, HumanMessage\n",
    "\n",
    "class AutomaticToolBinding:\n",
    "    \"\"\"Auto tool handling + manual tool execution + resumed response.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are a helpful assistant with tools. Use them when needed.\"),\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "            (\"human\", \"{question}\")\n",
    "        ])\n",
    "        self.bind_tools_chain = self.prompt |  llm_with_bind_tools.bind_tools(tools)\n",
    "        self.store = {}\n",
    "\n",
    "    def get_session_history(self, session_id: str) -> BaseChatMessageHistory:\n",
    "        if session_id not in self.store:\n",
    "            self.store[session_id] = InMemoryHistory()\n",
    "        return self.store[session_id]\n",
    "\n",
    "\n",
    "    def process_message(self, message: str, session_id: str = \"default\") -> str:\n",
    "\n",
    "        chain_with_memory = RunnableWithMessageHistory(\n",
    "            self.bind_tools_chain ,\n",
    "            self.get_session_history,\n",
    "            input_messages_key=\"question\",\n",
    "            history_messages_key=\"history\"\n",
    "        )\n",
    "\n",
    "        # Step 1: Get model's initial response\n",
    "        response = chain_with_memory.invoke(\n",
    "            {\"question\": message},\n",
    "            config={\"configurable\": {\"session_id\": session_id}}\n",
    "        )\n",
    "\n",
    "        tool_calls = response.additional_kwargs.get(\"tool_calls\", [])\n",
    "        history = self.get_session_history(session_id=session_id)\n",
    "\n",
    "        if tool_calls:\n",
    "            tool_messages = []\n",
    "\n",
    "            for call in tool_calls:\n",
    "                function_call = call['function']\n",
    "                # Correct way to get tool name and arguments\n",
    "                tool_name = function_call[\"name\"]\n",
    "                tool_args = json.loads(function_call[\"arguments\"])\n",
    "                tool_id = call[\"id\"]\n",
    "\n",
    "                # Run the tool manually with keyword args unpacked\n",
    "                if tool_name == \"calculator_tool\":\n",
    "                    output = calculator_tool.invoke(input=tool_args)\n",
    "                elif tool_name == \"get_current_time\":\n",
    "                    output = get_current_time.invoke(input=tool_args)\n",
    "                else:\n",
    "                    output = \"Tool not found.\"\n",
    "\n",
    "                # Wrap tool output into ToolMessage\n",
    "                tool_msg = ToolMessage(tool_call_id=tool_id, content=str(output))\n",
    "                tool_messages.append(tool_msg)\n",
    "                history.add_message(tool_msg)\n",
    "\n",
    "            # Step 2: Feed tool outputs back into model\n",
    "            followup = llm_with_bind_tools.invoke([\n",
    "                HumanMessage(content=message),\n",
    "                response,\n",
    "                *tool_messages\n",
    "            ])\n",
    "            return followup.content\n",
    "        else:\n",
    "            return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rt5lJSzaDHUw",
    "outputId": "84e1066a-6904-4f56-87d4-3a9a8964337e"
   },
   "outputs": [],
   "source": [
    "bind_tools_chain_with_memory = AutomaticToolBinding()\n",
    "# Test 1: Date query\n",
    "print(\"Query: What's today's date?\")\n",
    "response = bind_tools_chain_with_memory.process_message(message =\"What's date\", session_id =\"manual_session\")\n",
    "print(f\"Response: {response}\\n\")\n",
    "\n",
    "# Test 2: Math query\n",
    "print(\"Query: What's 4+4?\")\n",
    "response = bind_tools_chain_with_memory.process_message(message =\"What's 4+4?\", session_id =\"manual_session\")\n",
    "print(f\"Response: {response}\\n\")\n",
    "\n",
    "# Non tool call\n",
    "print(\"Query: Add 4 to previous answer\")\n",
    "response = bind_tools_chain_with_memory.process_message(message =\"Add 4 to previous answer\", session_id =\"manual_session\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ai-interview-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
