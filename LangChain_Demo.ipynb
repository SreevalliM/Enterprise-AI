{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1A-TPbE-wFgm"
   },
   "source": [
    "### Welcome to the LangChain demo!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAzkFldpwFgn"
   },
   "outputs": [],
   "source": [
    "# Load the environment variables\n",
    "import dotenv, os\n",
    "dotenv.load_dotenv()\n",
    "print(os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJnwyuH8wFgo"
   },
   "source": [
    "### Basic Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Mb9Jmk8wFgo"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gpt-5-mini\", model_provider=\"openai\", reasoning_effort=\"minimal\")\n",
    "model.invoke(\"Hello AI overlord!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfYWYuDewFgo"
   },
   "source": [
    "### Message Types\n",
    "Explicit construction of messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AAmskOhKwFgo"
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English into Italian\"),\n",
    "    HumanMessage(\"I'm excited to get started with the problem-first course!\"),\n",
    "]\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVH1GIT4wFgp"
   },
   "source": [
    "### Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ixMPvQkmwFgp"
   },
   "outputs": [],
   "source": [
    "# Messages\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_message = \"You are a wise and mystical AI fortune teller. Your predictions are funny, slightly exaggerated, but insightful. Keep it to 1-2 sentences\"\n",
    "user_message = \"Please entertain the user with their fortune request: {question}\"\n",
    "prompt_template = ChatPromptTemplate.from_messages([(\"system\", system_message), (\"user\", user_message)])\n",
    "\n",
    "# Construct the prompt from the template:\n",
    "question = \"What is the next trillion dollar idea?\"\n",
    "prompt = prompt_template.invoke({\"question\": question})\n",
    "\n",
    "# Check what's in the prompt\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FyrIQ3DnwFgp"
   },
   "outputs": [],
   "source": [
    "# Send the prompt to the model and get the result\n",
    "result = model.invoke(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmnF-shbwFgq"
   },
   "outputs": [],
   "source": [
    "## Output parser to convert it to a text:\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "parser_result = output_parser.invoke(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(parser_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DZ8jP7AwFgq"
   },
   "source": [
    "### Chaining\n",
    "Let's chain the output of the first model to the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CtG87-InwFgq"
   },
   "outputs": [],
   "source": [
    "# Remember: Use the template here directly in the chain instead of 'prompt'\n",
    "first_chain = prompt_template | model | output_parser\n",
    "first_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Uva2xPUwFgq",
    "outputId": "3758df7f-d13e-4fbe-9aa8-1e44e4a77082"
   },
   "outputs": [],
   "source": [
    "# Create 2nd prompt template:\n",
    "system_message = \"You are a stand-up comedian who tells hilarious jokes in a casual, witty style using AI terminology. Keep it short\"\n",
    "user_message = \"Tell a joke about this fortune telling: {fortune}.\"\n",
    "prompt_template_2 = ChatPromptTemplate.from_messages([(\"system\", system_message), (\"user\", user_message)])\n",
    "\n",
    "# Chain the 1st and 2nd prompts\n",
    "new_chain = prompt_template | model | output_parser | (lambda x: {\"fortune\": x}) | prompt_template_2 | model | output_parser\n",
    "new_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above is equivalent to running:\n",
    "first_chain = prompt_template | model | output_parser\n",
    "first_result = first_chain.invoke({\"question\": question})\n",
    "\n",
    "second_chain = prompt_template_2 | model | output_parser\n",
    "final_result = second_chain.invoke({\"fortune\": first_result})\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
