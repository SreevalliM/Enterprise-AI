# ðŸ§  DeepSeek-R1 and Large Reasoning Models

## Introduction: What Are LRMs?

Large Reasoning Models (LRMs) represent the next step in AI evolution, moving beyond simple text generation into **structured reasoning and decision-making**. Unlike LLMs, which predict text based on patterns, LRMs **simulate reasoning**, tackle complex problem-solving, perform multi-step inference, and make decisions consistently.  

**DeepSeek** is a recent innovation in this space.

---

## What Issues Do LRMs Solve in LLMs, and at What Cost?

LLMs have limitations in **logical consistency, hallucinations, and long-horizon decision making**. LRMs address these challenges:

1. **Logical Consistency:**  
   LRMs engage in self-verification and multi-step reasoning.  
   *Cost:* Higher computational power.

2. **Reducing Hallucinations:**  
   Incorporates structured knowledge retrieval and cross-checking outputs.  
   *Cost:* Longer generation times.

3. **Improving Long-Horizon Decision Making:**  
   Maintains logical consistency over multiple steps, using memory and planning frameworks.  
   *Cost:* Higher training and inference expenses.

> LRMs are resource-intensive but excel in scenarios demanding **high reasoning performance**.

---

## Architectural Differences: How LRMs Are Built Differently

### Pretraining: A Shift Toward Reasoning Data
- LLMs: Pretrained on internet-scale corpora for fluency.  
- LRMs: Integrate structured reasoning datasets (math proofs, logic exercises, chain-of-thought examples).

### Supervised Fine-Tuning (SFT): Teaching Explicit Reasoning Steps
- LLMs: Fine-tuned on instruction-following datasets.  
- LRMs: Fine-tuned on multi-step reasoning datasets, learning to justify answers and cross-validate outputs.

### Reinforcement Learning (RL): Optimizing for Logical Correctness
- LLMs: RLHF focuses on fluency and coherence.  
- LRMs: RL rewards **logical correctness**, with self-evaluation loops for robust outputs.

---

## How LRMs Differ from LLMs: The Input-Output Framework

### Input Differences
- LLMs: Require explicit prompting for reasoning (e.g., "think step by step").  
- LRMs: Naturally engage in multi-step logical analysis.

### Output Differences
- LLMs: Predict next word; may hallucinate or contradict themselves.  
- LRMs: Generate **structured reasoning steps** with verified and coherent outputs.

---

## DeepSeekâ€™s Approach: Innovations in LRM Training

### DeepSeek-R1-Zero
- Skips supervised fine-tuning; uses RL directly on the base model.  
- Develops self-verification and reflection independently.

### DeepSeek-R1
- Multi-stage training pipeline:  
  1. Cold-start high-quality data  
  2. Two rounds of RL for complex reasoning patterns  
  3. Two rounds of SFT to reinforce language skills and reasoning justification

> DeepSeek-R1 balances **low cost and high performance**, driving its popularity.

---

## Recommended Resources
- [The Illustrated DeepSeek-R1](https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1) 
- [Understanding Reasoning LLMs](https://sebastianraschka.com/blog/2025/understanding-reasoning-llms.html)

---

